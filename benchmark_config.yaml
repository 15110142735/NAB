# The data/ directory is subdivided into different groups of datasets which
# you may want to run independently. You may specify any combination of
# those groups here and run_benchmark.py will automatically use those datasets.
DataGroups:
    - artificialWithAnomaly
    - artificialNoAnomaly
    - realAWSCloudwatch

# We provide two anomaly detectors. You may want to use one or both of them
# to generate results. You may also create your own detector as a subclass of
# AnomalyDetector and then add it here to have run_benchmark.py use your
# detector to generate results.
AnomalyDetectors:
    - grok
    - skyline

ResultsDirectory: "results"

# These values have been picked to reflect realistic costs of reacting to
# each type of event for the server monitoring data which comprise the
# NAB corpus. 
#
# NOTE: Results from running the benchmark with modified costs are NOT directly
# comparable to those on the leaderboard.
CostMatrix:
    tpCost: 0.0
    fnCost: 200.0
    fpCost: 50.0
    tnCost: 0.0
