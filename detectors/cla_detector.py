import os
import simplejson as json

from nupic.algorithms import anomaly_likelihood
from nupic.frameworks.opf.modelfactory import ModelFactory

from anomaly_detector import AnomalyDetector

class CLADetector(AnomalyDetector):

  def __init__(self, minVal, maxVal, *args, **kwargs):

    # Load the model params JSON
    paramsPath = os.path.join(os.path.split(__file__)[0],
                "modelParams",
                "model_params_rdse_94.json")
    with open(paramsPath) as fp:
      modelParams = json.load(fp)

    self.sensorParams = modelParams['modelParams']['sensorParams']\
                                   ['encoders']['value']
    
    # # RDSE - resolution calculation
    resolution = max(0.001,
                     (maxVal - minVal) / self.sensorParams.pop('numBuckets')
                    )
    self.sensorParams['resolution'] = resolution

    # Scalar - update the min/max value for the encoder
    # self.sensorParams['minval'] = minVal
    # self.sensorParams['maxval'] = maxVal
    
    self.model = ModelFactory.create(modelParams)

    self.model.enableInference({'predictedField': 'value'})

    # The anomaly likelihood object
    self.anomalyLikelihood = AnomalyLikelihood()

    # Init the super class
    super(CLADetector, self).__init__(*args, **kwargs)

  def getOutputPrefix(self):
    """
    Returns the string to prepend to results files generated by this class
    """
    return "cla"

  def getAdditionalHeaders(self):
    """
    Returns a list of strings.
    """

    return ["_raw_score"]

  def handleRecord(self, inputData):
    """
    Returns a list [anomalyScore, rawScore].

    Internally to NuPIC "anomalyScore" corresponds to "likelihood_score"
    and "rawScore" corresponds to "anomaly_score". Sorry about that.
    """

    # Send it to the CLA and get back the results
    result = self.model.run(inputData)
    
    # Retrieve the anomaly score and write it to a file
    rawScore = result.inferences['anomalyScore']

    # Compute the Anomaly Likelihood
    anomalyScore = self.anomalyLikelihood.likelihood(inputData["value"],
                                                     rawScore,
                                                     inputData["timestamp"])

    return [anomalyScore, rawScore]

#############################################################################

class AnomalyLikelihood(object):
  """
  Helper class for running anomaly likelihood computation.
  """
  
  def __init__(self, probationaryPeriod = 600, CLALearningPeriod = 300):
    """
    CLALearningPeriod - the number of iterations required for the CLA to
    learn some of the patterns in the dataset.
    
    probationaryPeriod - no anomaly scores are reported for this many
    iterations.  This should be CLALearningPeriod + some number of records
    for getting a decent likelihood estimation.
    
    """
    self._iteration          = 0
    self._historicalScores   = []
    self._distribution       = None
    self._probationaryPeriod = probationaryPeriod
    self._CLALearningPeriod  = CLALearningPeriod


  def _computeLogLikelihood(self, likelihood):
    """
    Compute a log scale representation of the likelihood value. Since the
    likelihood computations return low probabilities that often go into 4 9's or
    5 9's, a log value is more useful for visualization, thresholding, etc.
    """
    # The log formula is:
    # Math.log(1.0000000001 - likelihood) / Math.log(1.0 - 0.9999999999);
    return math.log(1.0000000001 - likelihood) / -23.02585084720009


  def likelihood(self, value, anomalyScore, dttm):
    """
    Given the current metric value, plus the current anomaly
    score, output the anomalyLikelihood for this record.
    """
    dataPoint = (dttm, value, anomalyScore)
    # We ignore the first probationaryPeriod data points
    if len(self._historicalScores) < self._probationaryPeriod:
      likelihood = 0.5
    else:
      # On a rolling basis we re-estimate the distribution every 100 iterations
      if self._distribution is None or (self._iteration % 100 == 0): 
        _, _, self._distribution = (
          anomaly_likelihood.estimateAnomalyLikelihoods(
            self._historicalScores,
            skipRecords = self._CLALearningPeriod)
          )
        
      likelihoods, _, self._distribution = (
        anomaly_likelihood.updateAnomalyLikelihoods([dataPoint],
          self._distribution)
      )
      likelihood = 1.0 - likelihoods[0]
      
    # Before we exit update historical scores and iteration
    self._historicalScores.append(dataPoint)
    self._iteration += 1

    return likelihood